# -*- coding: utf-8 -*-
"""Youtube_API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19TEDeaWPoG1FuIq_NPjtWKHcc0cAwcwH
"""

pip install pandas

pip install matplotlib

pip install google-api-python-client

pip install seaborn

import matplotlib.pyplot as plt
import seaborn as sns

import os
import googleapiclient.discovery
import pandas as pd


api_key = 'AIzaSyC2fVeX1fHr5DMA4ygjdCqjkLKEcZMrWXs'  # Replace 'YOUR_API_KEY' with your actual API key
youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)

channel_id ='UCvC4D8onUfXzvjTOM-dBfEA'

request = youtube.channels().list(part = 'snippet,contentDetails,statistics', id = channel_id)
request.execute()

from datetime import datetime

region_code = 'IN'  # For Indian videos
max_results = 50  # Maximum number of videos to retrieve per request




category_names = {
    '1': 'Film & Animation',
    '2': 'Autos & Vehicles',
    '10': 'Music',
    '15': 'Pets & Animals',
    '17': 'Sports',
    '18': 'Short Movies',
    '19': 'Travel & Events',
    '20': 'Gaming',
    '21': 'Videoblogging',
    '22': 'People & Blogs',
    '23': 'Comedy',
    '24': 'Entertainment',
    '25': 'News & Politics',
    '26': 'Howto & Style',
    '27': 'Education',
    '28': 'Science & Technology',
    '29': 'Nonprofits & Activism',
    '30': 'Movies',
    '31': 'Anime/Animation',
    '32': 'Action/Adventure',
    '33': 'Classics',
    '34': 'Comedy',
    '35': 'Documentary',
    '36': 'Drama',
    '37': 'Family',
    '38': 'Foreign',
    '39': 'Horror',
    '40': 'Sci-Fi/Fantasy',
    '41': 'Thriller',
    '42': 'Shorts',
    '43': 'Shows',
    '44': 'Trailers'
}

# Fetch trending videos
trending_videos_request = youtube.videos().list(
    part='snippet,statistics',
    chart='mostPopular',
    regionCode=region_code,
    maxResults=max_results
)
trending_videos_response = trending_videos_request.execute()
trending_videos = trending_videos_response['items']

# print(trending_videos_response)


# Process the retrieved data
# Process the retrieved data
# Process the retrieved data
video_data = []
for video in trending_videos:
    video_id = video['id']
    title = video['snippet']['title']
    views = int(video['statistics']['viewCount'])

    # Check if 'likeCount' key exists in the statistics dictionary
    if 'likeCount' in video['statistics']:
        likes = int(video['statistics']['likeCount'])
    else:
        likes = 0  # Assign a default value if like count is not available

    # Check if 'dislikeCount' key exists in the statistics dictionary
    if 'dislikeCount' in video['statistics']:
        dislikes = int(video['statistics']['dislikeCount'])
    else:
        dislikes = None

    comment_count = int(video['statistics']['commentCount'])
    title_length = len(title)
    channel_title = video['snippet']['channelTitle']
    category_id = video['snippet']['categoryId']
    category_name = category_names.get(category_id, 'Unknown')
    publish_date = video['snippet']['publishedAt']
    upload_time =  datetime.strptime(publish_date, '%Y-%m-%dT%H:%M:%SZ').time()
    publish_date = datetime.strptime(publish_date, '%Y-%m-%dT%H:%M:%SZ').date()

    # Append video data to the list
    video_data.append({
        'video_id': video_id,
        'title': title,
        'views': views,
        'likes': likes,
        'dislikes': dislikes,
        'comment_count': comment_count,
        'title_length': title_length,
        'channel_title': channel_title,
        'category_name': category_name,
        'publish_date': publish_date,
        'upload_time' : upload_time
    })




# Convert the list of dictionaries into a DataFrame
video_df = pd.DataFrame(video_data)
video_df

total_views = video_df['views'].sum()
print("Total views of trending videos:", total_views)

# Calculate summary statistics
mean_views = video_df['views'].mean()
median_views = video_df['views'].median()
quartiles_views = video_df['views'].quantile([0.25, 0.5, 0.75])

print("Mean views:", mean_views)
print("Median views:", median_views)
print("Quartiles:")
print(quartiles_views)

# Visualize the distribution of views
plt.figure(figsize=(10, 6))
sns.histplot(video_df['views'], bins=30, kde=True)
plt.xlabel('Views')
plt.ylabel('Frequency')
plt.title('Distribution of Views among Trending Videos')
plt.show()

# Calculate summary statistics for likes
mean_likes = video_df['likes'].mean()
median_likes = video_df['likes'].median()
quartiles_likes = video_df['likes'].quantile([0.25, 0.5, 0.75])

# Calculate summary statistics for comments
mean_comments = video_df['comment_count'].mean()
median_comments = video_df['comment_count'].median()
quartiles_comments = video_df['comment_count'].quantile([0.25, 0.5, 0.75])

print("Likes:")
print("Mean likes:", mean_likes)
print("Median likes:", median_likes)
print("Quartiles for likes:")
print(quartiles_likes)

print("\nComments:")
print("Mean comments:", mean_comments)
print("Median comments:", median_comments)
print("Quartiles for comments:")
print(quartiles_comments)

# Visualize the distribution of likes
plt.figure(figsize=(10, 6))
sns.histplot(video_df['likes'], bins=30, kde=True, color='green')
plt.xlabel('Likes')
plt.ylabel('Frequency')
plt.title('Distribution of Likes among Trending Videos')
plt.show()

# Visualize the distribution of comment counts
plt.figure(figsize=(10, 6))
sns.histplot(video_df['comment_count'], bins=30, kde=True, color='blue')
plt.xlabel('Comment Count')
plt.ylabel('Frequency')
plt.title('Distribution of Comment Count among Trending Videos')
plt.show()

# Assuming you have the publication dates of the videos in the 'publish_date' column
# Convert 'publish_date' to datetime format if it's not already
video_df['publish_date'] = pd.to_datetime(video_df['publish_date'])

# Calculate the duration each video remained on the trending list
current_date = datetime.now()  # Current date
video_df['days_on_trending'] = (current_date - video_df['publish_date']).dt.days

# Now you can find the video with the longest duration on the trending list
longest_duration_video = video_df.loc[video_df['days_on_trending'] == video_df['days_on_trending'].max()]

print("Longest trending video:")
print(longest_duration_video[['title', 'days_on_trending']])

import re

# Assuming video titles are stored in the 'title' column of the DataFrame
titles = video_df['title']

# Define a regular expression pattern to match fully capitalized words
pattern = r'\b[A-Z]+\b'

# Initialize a list to store fully capitalized words in titles
fully_capitalized_words = []

# Iterate through each title and find fully capitalized words
for title in titles:
    capitalized_matches = re.findall(pattern, title)
    fully_capitalized_words.extend(capitalized_matches)

# Convert the list to a set to remove duplicate words
fully_capitalized_words = set(fully_capitalized_words)

# Print the fully capitalized words found in video titles
print("Fully capitalized words in video titles:")
print(fully_capitalized_words)

# Create a DataFrame with video titles and their lengths
title_lengths_df = pd.DataFrame({
    'Title': video_df['title'],
    'Title Length': video_df['title'].apply(len)
})

# Print the DataFrame containing titles and their lengths
print("Trending Video Titles and Lengths:")
print(title_lengths_df)

# Calculate summary statistics for title lengths
mean_title_length = title_lengths_df['Title Length'].mean()
median_title_length = title_lengths_df['Title Length'].median()
max_title_length = title_lengths_df['Title Length'].max()
min_title_length = title_lengths_df['Title Length'].min()

print("\nSummary Statistics for Title Lengths:")
print("Mean Title Length:", mean_title_length)
print("Median Title Length:", median_title_length)
print("Maximum Title Length:", max_title_length)
print("Minimum Title Length:", min_title_length)

# Visualize the distribution of title lengths
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
title_lengths_df['Title Length'].hist(bins=30, color='skyblue', edgecolor='black', alpha=0.7)
plt.xlabel('Title Length')
plt.ylabel('Frequency')
plt.title('Distribution of Trending Video Title Lengths')
plt.grid(False)
plt.show()

# Select relevant columns for correlation analysis
correlation_data = video_df[['views', 'likes', 'dislikes', 'comment_count', 'title_length']]

# Calculate the correlation matrix
correlation_matrix = correlation_data.corr()
correlation_matrix_percentage = correlation_matrix * 100

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix_percentage, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title('Correlation Matrix of Video Attributes')
plt.show()

"""That means there is 52.8% chance for the videos to get more views by getting more comments and vice versa"""

from collections import Counter
import nltk
from nltk.corpus import stopwords

# Download the NLTK stopwords corpus if you haven't already
nltk.download('stopwords')

# Get the list of English stopwords
stop_words = set(stopwords.words('english'))

# Tokenize the titles and count the occurrences of each word
all_words = ' '.join(video_df['title']).split()
filtered_words = [word.lower() for word in all_words if word.lower() not in stop_words and len(word) > 2]  # Filter out stopwords and short words

# Count the occurrences of each word
word_counts = Counter(filtered_words)

# Display the most common words
most_common_words = word_counts.most_common(10)
print("Most common words in trending video titles:")
for word, count in most_common_words:
    print(f"{word}: {count}")

# Count the occurrences of each channel title
channel_counts = video_df['channel_title'].value_counts()

# Display the channels with the largest number of trending videos
top_channels = channel_counts.head(10)  # Assuming you want to display the top 10 channels
print("YouTube channels with the largest number of trending videos:")
print(top_channels)

# Count the occurrences of each video category
category_counts = video_df['category_name'].value_counts()

# Display the category with the largest number of trending videos
largest_category = category_counts.idxmax()
print("Video category with the largest number of trending videos:")
print(largest_category)

# Assuming 'video_df' is your DataFrame containing information about trending videos

titles = []
publish_dates = []
day = []
uploaded_Time = []


# Iterate through the top 10 trending videos
for index, video in video_df.head(10).iterrows():

    titles.append(video['title'])
    publish_dates.append(video['publish_date'])
    day.append(video['publish_date'])
    uploaded_Time.append(video['upload_time'])

# Create a DataFrame to store the data
data = {
    'Title': titles,
    'Publish Date': publish_dates,
    'Day': day,
    'Uploaded Time': uploaded_Time
}
top_10_df = pd.DataFrame(data)

top_10_df